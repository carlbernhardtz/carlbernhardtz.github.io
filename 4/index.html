<!DOCTYPE html>
<html lang="en">
<head>    
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 1</title>
    <style>
        body {
            font-family: Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #ffffff;
        }
    
        .container {
            max-width: 800px; /* Limit the width of the content to 800px */
            margin: 40px auto; /* Center the content horizontally */
            padding: 20px;
            background-color: white;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Adds a subtle shadow */
            border-radius: 8px;
            text-align: center;
        }
    
        h1, h2, h3, h4 {
            text-align: left; /* Center all headers */
            margin-left: 250px;
            margin-bottom: 40px;
            margin-top: 40px;
        
        }
    
        p {
            line-height: 1.6;
            text-align: left;
            margin-bottom: 20px;
            margin-left: 300px;
            margin-right: 300px;
        }
        .image-gallery {
            display: flex;
            justify-content: center;
            flex-wrap: nowrap; /* Prevent wrapping to a new row */
            gap: 5px; /* Adds space between the images */
        }
        .image-item {
            text-align: center;
            flex-basis: auto; /* Allow the image to take its natural size */
            margin: 40px; /* Ensure no additional margin is added */
        }
        img {
            max-width: 100%; /* Ensure images scale down but not exceed their container */
            /*height: auto;*/
        }
        figure {
            display: inline-block; /* Ensures figure and image stay close together */
            margin: 0; /* Remove any default margin */
            padding: 0; /* Remove any default padding */
        }
        figcaption {
            margin-top: 5px;
            text-align: center;
            font-size: large;
        }
        .feature-patches-caption {
            margin-top: -60px; /* Adjust this value to control the spacing */
        }
    </style>

    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Project 4: IMAGE WARPING and MOSAICING</h1>
    <h1>Part A</h1>
    <p>The goal of this project was to warp images and change their perspective and subsequently create a panoramic looking image.
    
    <h2>Recover Homographies</h2>

    <p>
        The <code>computeH</code> function calculates the homography matrix \( H \) that maps points from one image to another using 
        corresponding point pairs. These correspondences were made using the provided online tool.
        </p>
        
        <p>
        For each point pair \( (x_1, y_1) \) in <code>im1_pts</code> and \( (x_2, y_2) \) in <code>im2_pts</code>, we append two rows to matrix \( A \):
        </p>
        
        <p>
        $$
        \begin{pmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 & -x_1 x_2 & -y_1 x_2 \end{pmatrix}
        $$
        $$
        \begin{pmatrix} 0 & 0 & 0 & x_1 & y_1 & 1 & -x_1 y_2 & -y_1 y_2 \end{pmatrix}
        $$
        </p>
        
        <p>
        We solve for \( h \) using least squares and reshape it into the final homography matrix:
        </p>
        
        <p>
        $$
        H = \begin{pmatrix} 
        h_{11} & h_{12} & h_{13} \\
        h_{21} & h_{22} & h_{23} \\
        h_{31} & h_{32} & 1 
        \end{pmatrix}
        $$
        </p>

    <p>

    <h2>Image Rectification</h2>

    <p>The next step was to use the homography to rectify images. This was done by taking a picture if something, slightly skewed. Then points were
        selected using the online tool. With the computed homography, the image could then be warped by mapping the keypoints to the new location. 
        The warping was done using an inverse warp along with some interpolation. The results are presented down below. 
    </p>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mac.jpeg"  height="600">
            <figcaption>Original image of MacBook</figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/mac_rect.jpeg"  height="600">
            <figcaption>Rectified image of a MacBook</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/rub.jpeg"  height="600">
            <figcaption>Original image of a Rubik's Cube</figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/rub_rect.jpeg"  height="600">
            <figcaption>Rectified image of a Rubik's Cube</figcaption>
        </figure>
    </div>

    <h2>Image Mosaicing</h2>

    <p>
        The mosaic blending process combines two images into one by warping one image to align with the other and creating a
         transition between them. The steps involved are as follows:
        </p>
        
        <h4>1. Warping the First Image</h4>
        <p>
        The first image is transformed using a homography matrix, which aligns it with the perspective of the second image. After warping,
         the bounding box for both images is determined. This ensures the final output can hold both images by calculating the minimal and
          maximal coordinates of the two images.
        </p>
        
        <h4>2. Preparing images</h4>
        <p>
        Two empty images are created, large enough to contain both the warped and non-warped images. The warped image is placed into the first 
        image, and the non-warped image is placed into the second image, with correct offsets applied to ensure proper alignment. This was by far 
        the most time consuming and frustrating part of this project.
        </p>
        
        <h4>3. Creating a Transition</h4>
        <p>
        A blending mask is generated to handle the transition between the two images. This is done by creating binary masks of the non-empty regions
         of both images. Then, the distance of each pixel from the nearest edge is calculated to create a gradual transition between the two images.
          The distances are normalized to produce a transition mask, which determines how the two images will blend together.
        </p>
        
        <h4>4. Blending the Images</h4>
        <p>
        The blending mask is applied to both images, ensuring that the two images transition in the overlapping region. The pixels from
         each image are combined based on the values of the mask, resulting in a blend where the images overlap.
        </p>
        
        <h4>5. Final Mosaic</h4>
        <p>
        The final mosaic is created by combining the two images into one, with a transition in the overlapping regions, producing a visually
         cohesive result without too many noticeable artifacts.
        </p>
        
    <h2>Results</h2>

    <p>Three different mosaics were created. There are still some small artifacts left. In the mosaic of my room, the exposure in the two images 
    were quite different and therefore the blending was not perfect. In the other two mosaics you can see some slight blurring where the 
    images are stitched together. But all in all, an ok result in my opinion. The source images and the results are presented down below.
</p>
    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mof1.jpeg"  width="500">
            <figcaption>Image before in Moffitt Library </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/mof2.jpeg"  width="500">
            <figcaption>Second image in Moffitt Library</figcaption>
        </figure>
    </div>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_mof.jpeg"  width="900">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/out1.jpeg"  width="500">
            <figcaption>Image before warping from my bedroom window </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/out2.jpeg"  width="500">
            <figcaption>Second image from my bedroom window</figcaption>
        </figure>
    </div>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_out.jpeg"  width="900">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>


    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/roof1.jpeg"  width="500">
            <figcaption>Image before warping from the roof </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/roof2.jpeg"  width="500">
            <figcaption>Second image from the roof</figcaption>
        </figure>
    </div>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_roof.jpeg"  width="900">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>

    <h1>Part B</h1>
    <h2>Corner Detection</h2>

    <p>First, the given code for Harris corner detection was run in order to compute the interest points in the image. These interest points
        were way too many which is displayed below.
    </p>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/harris_c.png"  width="900">
            <figcaption>Harris corner</figcaption>
        </figure>
    </div>

    <h2>Adaptive Non-maximal Suppression (ANMS)</h2>
    <p>
        This approach uses the adaptive non-maximal suppression (ANMS) algorithm to select well-separated interest points.
        It calculates a distance, \( r_i \), for each interest point \( \vec{x}_i \) to its nearest point \( \vec{x}_j \) with a significantly
         higher corner response. Specifically, it finds the minimum distance to any \( \vec{x}_j \) such 
         that \( h(\vec{x}_i) < c_{\text{robust}} \cdot h(\vec{x}_j) \). Only the top-K points with the largest \( r_i \) values are kept, 
         ensuring that interest points are not clustered together but spread evenly across the image. The result is displayed below.
    </p>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/anms_pts.png"  width="900">
            <figcaption>Keypoints with ANMS</figcaption>
        </figure>
    </div>

    <h2>Feature Extraction</h2>
    <p>Using the calculated interest points, feature patches were extracted for each points. Patches were taken around specific interest
         points in the image. Each patch was centered on the point and sized to avoid interference from edge effects. To standardize the
          patches, their intensity values were normalized by adjusting for mean and variance, ensuring consistent contrast.
           Each patch was then resized to a smaller, uniform scale, making the data compact and optimized for subsequent processing and 
           comparison. The patches are displayed below.</p>


        <div class="image-gallery">
            <figure class="image-item">
                <img src="media/patches.png"  width="600">
                <figcaption class="feature-patches-caption">Feature Patches</figcaption>
            </figure>
        </div>

    <h2>Feature Matching</h2>
        <p>Having the feature extraction in place, it was now time to match features in two images. For each patch in the first image, distances
             to patches in the second were calculated based on the sum of squared differences (SSD). To ensure reliable matches, Lowe's ratio
              test was applied, comparing the closest and second-closest matches for each feature. This filtering step helped retain only
               robust matches, which were then output as corresponding coordinates in both images. The result is displayed below.</p>

               <div class="image-gallery">
                <figure class="image-item">
                    <img src="media/feat_match.png"  width="900">
                    <figcaption>Matching Features</figcaption>
                </figure>
                </div>

    <h2>RANSAC</h2>
    <p>The feature matching may seem solid but there can still be outliers present. Therefore we turn to RANSAC - 
        <em>Random sample consensus</em>. This is used to ensure a robust transformation by handling outliers in the matched points. By 
        iteratively selecting random subsets of matches to compute possible homographies, RANSAC identifies the transformation with the most
         inliers, discarding mismatches. This approach yields an accurate homography that aligns the images, even when only a subset of matches
        is correct. The difference when using RANSAC and not is displayed below.
    </p>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/feat_match.png"  width="600">
            <figcaption>Matched points without using RANSAC </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/ransac_pts.png"  width="600">
            <figcaption>Matched points using RANSAC</figcaption>
        </figure>
    </div>

    <h2>Results</h2>
    <p>The results for the manual mosaicing and the "automatic" keypoint detection and stitching are displayed alongside each other down below.
    </p>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_mof.jpeg"  width="600">
            <figcaption>Manual Mosaic </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/mof_stitch.jpeg"  width="600">
            <figcaption>Auto-stitched Mosaic</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_out.jpeg"  width="600">
            <figcaption>Manual Mosaic </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/out_stitch.jpeg"  width="600">
            <figcaption>Auto-stitched Mosaic</figcaption>
        </figure>
    </div>

    <div class="image-gallery">
        <figure class="image-item">
            <img src="media/mosaic_roof.jpeg"  width="600">
            <figcaption>Manual Mosaic </figcaption>
        </figure>
        <figure class="image-item">
            <img src="media/roof_stitch.jpeg"  width="600">
            <figcaption>Auto-stitched Mosaic</figcaption>
        </figure>
    </div>
    <p>There are some slight differences but the overall result is largely the same. In then first mosaic there is some blurring at the bottom 
        where the images are stitched. This is better in the automated one. The same goes for the last mosaic where some blurring occurs at the 
        seam in the manual one where the automatic one handles it better. The automatic corner detection, stitching etc. is 
        thus in my opinion, deemed successful. The coolest thing about this project for me is the automatization of processes. Being able to 
    choose the correct keypoints without manual labor is really fascinating.</p>

    <p><a href="../index.html">Back to Main Page</a></p>
</body>
</html>
